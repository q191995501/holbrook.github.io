Title: 《量化投资：数据挖掘技术与实践(MatLab版)》读书笔记第二章：数据挖掘的内容、过程及工具
Date: 2017-07-30
Category: 量化交易
Tags: 数据分析
Summary: 数据挖掘一般分为关联、回归、分类、聚类、预测和诊断。
数据挖掘的过程包括定义目标、准备数据、建立模型、评估模型、部署模型六个阶段。


# 数据挖掘的内容

对数据的挖掘，主要有以下几种方法。

- 关联

  两个或多个变量的值之间存在某种规律性，就称之为关联。关联可以分为简单关联、时序关联和因果关联。
  而关联分析就是找出数据之间隐藏的关联关系。

  有时候，关联关系存在某种不确定性，可以通过可信度来描述其关联程度。

  关联规则挖掘，就是通过分析数据，发现数据项集之间的关联关系或相关联系。主要有几种情况：

  + 类别数据之间的关联，其关联关系为bool型和数值型。比如，`性别=女` 和 `职业=秘书` 之间的关联；
    或者 `性别=女` 和 `平均输入=2300` 之间的关联。

  + 层次数据之间的关联，可以分为单层关联规则和多层关联规则。比如 `IBM台式机` 和 `Sony打印机`
    是细节数据之间的单层关联规则； `台式机` 和 `Sony打印机` 是高层数据和细节数据之间的多层关联规则。

  + 基于数据的维数，可以分为单维和多维关联规则。单维关联规则处理同一属性中的关系，
    比如 `啤酒` 和 `尿布` 同属于 `顾客购买的商品` 这一属性； 多维关联规则处理多个属性中的关系，
    比如`性别=女` 和 `职业=秘书` 处理`性别`和`职业`两个属性之间的关系。

  量化投资中的关联规则，通常是上述三种情况的某一种，可以按照这些思路去寻找关联关系。

  关联分析的算法主要包括 Apriori, FP-Tree 和 HotSpot，会在后面具体介绍。

- 回归

  回归(Regression)是指确定两种货两种以上变量之间相互定量关系的统计分析方法，
  回归是数据挖掘中最基础、应用最多的方法。

  在量化投资领域，通常用回归方法研究经济周氏、大盘走势、个股走势。比如，常用的多因子模型就可以用回归方法得到。

  根据回归函数的类型，可以将回归方法分为现象回归、非线性回归、逐步回归（在回归过程中可以在调整变量数)、
  Logistic回归(以指数函数作为回归模型)。

  根据相关变量的个数，可以分为一元回归和多元回归。

- 分类

  数据挖掘中的分类，是指根据数据的特征，将事物划分类别。

  目前主要的分类方法有：

  + KNN(最近邻)
  + 贝叶斯
  + 神经网络
  + Logistic
  + 判别分析
  + SVM（支持向量机）
  + 决策树（主要包括 ID3、 C4.5 、 CART等算法）

- 聚类

  聚类分析(Cluster Analysis)，是根据“物以类聚”的道理，对样品进行分类的一种多元统计分析方法。
  特指在没有先验知识的情况下，将大量的样品分为不同的簇，以满足同一个簇中的对象有很大相似性，
  而不同的簇中的对象有很大的差异性。

  聚类和分类可以结合使用：线用聚类将样品大致分成几类，再用分类方法对样品进行分类。也就是说，
  在没有先验知识的情况下，通过聚类来实现更合理的分类。

  聚类算法使用的比较多的有：

  + Kmeans
  + 层次聚类
  + 神经网络聚类
  + 模糊C均值聚类
  + 高斯聚类

- 预测

  数据挖掘中的预测(Forecasting)，是指采集历史数据，基于这些数据建立模型，并用于推算将来。

  预测可以分为定性预测和定量预测。定性预测方法包括专家会议法/Delphi法，主观概率法，领先指标法等。
  而这里的预测是指定量预测，主要分为时间序列分析和因果关系分析。

  时间序列分析的主要方法有：

  + 移动平均法
  + 指数平滑法
  + Box-Jenkins法

  因果关系分析的主要方法有：

  + 回归法
  + 计量经济模型
  + 神经网络预测法
  + 灰色系统模型
  + 马尔科夫预测

- 诊断

  数据挖掘中的诊断，是指发现离群点。离群点是指与其他数据部分不同或不一致的点，通常不符合数据模型。
  离群点可能由错误导致，也由可能包含重要的信息。比如，不寻常的行为可能意味着信用卡欺诈。

  经典的离群点诊断的算法可以分为以下几类：

  + 基于统计学的方法
  + 基于距离或邻近度的方法
  + 基于偏差的方法
  + 基于密度的方法
  + 基于聚类的方法

  还有一些新兴的离群点诊断算法，包括：

  + 基于关联的方法
  + 基于模糊集的方法
  + 基于人工神经网络的方法
  + 基于遗传算法
  + 基于克隆选择方法

# 数据挖据过程

一些厂商对数据挖掘的过程进行了抽象和定义，比如：

- SAS提出了SEMMA过程模型，包括Sample(抽样)、Explore（探索）、Manipulate(处理)、Model(建模)、Access(评估)五个阶段。
- SPSS提出了5A模型，即Assess(评估)、Access(访问)、Analyze(分析)、Act(行动)、Automate(自动化)。
- 商业项目普遍采用CRISP-DM(Cross-Industry Standard Process for Data Mining, 跨行业数据挖掘过程标准)，
  将数据挖掘项目分为六个阶段：业务理解、数据理解、数据准备、建模、评估、部署。

综合上述模型，本书提出DPEMED(Definition、Preparation、Explore、Modeling、Evaluation、Deployment模型。
即以下几个步骤。

## 目标的定义

要确定目标，首先要了解业务。包括如下三个方面：

1. 需要解决问题的明确定义
2. 对相关数据的了解
3. 数据挖掘结果对业务作用效力的预测

了解了业务之后就可以定义目标。可以从两个方面考虑：

1. 数据挖掘要解决的问题
2. 数据挖掘完成后的效果，最好给出关键的评估指标。比如，在3个月内提供整体收益5%

## 数据的准备

数据准备是数据挖掘中耗时最多的环节。包括三个环节：

- 数据的选择

  从内部数据和外部数据中选择出要使用的数据。一般会考虑三种类型：

  + 业务数据：即业务发生时产生的操作数据，一般有明显的时间和顺序特征，并且与业务发生有关联
  + 关系数据：表达客户、机构、业务之际的关系，相对变化较少
  + 统计数据：与业务相关的描述信息

  这三种数据反映了三种不同的信息，对知识的发现非常重要。所以选择数据室要尽量包含这三种类型的数据。

- 数据质量分析

  评估数据质量，并为下一步的预处理提供参考。通常考虑以下几个方面：

  + 缺失数据。比如空值
  + 数据错误
  + 度量标准错误
  + 编码不一致
  + 无效的元数据

- 数据预处理

  解决数据质量方面的问题。一般包括清洗、集成、规约、变换4个步骤。

## 数据的探索

探索数据，是对数据进行初步研究，了解数据的特征，为建模的变量选择和算法选择提供依据。

数据探索和数据准备可以交替进行，直到满足建模的要求。

数据探索没有一定的方法，但可以从以下几个方面入手：

- 描述统计

  如均值、频率、众数、百分位数、中位数、极差、方差等，用于探索数据的不同属性。

- 数据可视化

  将数据以图形的方式呈现，可以根据一定的规则（标准差、百分数等）进行拆分、合并等处理，以便发现其中的模式。

- 套用模型

  套用建模的统计方法或计量模型，看看数据是否符合模型。

## 模型的建立

模型的建立是数据挖掘的核心。其过程包括：

1. 对准备好的数据用辅助模型进行处理（称为数据降维，如主成分PCA, 因子分析等），得到数据挖掘模型
2. 根据问题的性质，确定模型类别。主要是关联、回归、分类、聚类、预测、诊断六大类模型
3. 针对问题的类别，选择合适的算法（要考虑到数据特征、挖掘经验、算法适应性等方面）

   其中，根据算法的人工干预程度，可以分为有监督模型和无监督模型。

   - 有监督模型
     + 分类（贝叶斯，SVM...)
     + 回归（线性，逐步...)
     + 预测（灰色，马尔科夫...)
   - 无监督模型
     + 聚类（Kmeans,层次...)
     + 关联（Apriori, FP-Growth...）
     + 诊断（统计，距离...）
4. 使用训练集数据，通过现有数据和已知结果训练模型，优化参数
5. 使用测试集数据（与训练集数据要分开），通过现有数据和已知结果，对模型进行验证。
   常用的验证方法包括简单验证、交叉验证、N维交叉验证。

   - 简单验证

     是最基本的验证方法。从原始数据中抽取5%--33%的数据作为测试数据（要符合抽样原则），
     验证模型预测的正确率(对于离散结果)或准确率（用方差描述，比如回归问题）。

     在模型建立过程中，简单验证通常要执行几十次，不断的训练-测试，直到结果不再提高。

   - 交叉验证

     交叉验证(Cross Validation)是验证模型性能的一种统计分析方法。其基本思想是：

     将原始数据集（DataSet)进行分组，一部分作为训练集（Train Set)，另一部分作为验证集（Validation Set)。
     先用训练集训练模型，再用验证集测试模型。根据划分方法的不同，可以分为三种情况：

     + 二分验证

       将原始数据分成两组即可。过于简单，结果没有说服力，一般不使用。

     + K-CV（K次交叉验证）

       将原始数据分成（一般均分）K组。分别用其中一组作为验证集，其他的组作为训练集，这样得到K个模型性能指标。
       将K次指标的平均值作为最终模型的指标。

       K-CV可以有效避免过学习或欠学习状态，结果也比较有说服力。

     + LOO-CV（留一交叉验证）

       与K-CV相同，只不过每组的样本个数为1。这样可以避免随机因素的影响。缺点是计算量很大。

   - N维交叉验证

     ？

## 模型的评估

评估模型不仅仅是预测精确度，还有LIFT、ROC、Gain图等。

在量化投资领域。。。 回报率、最大回撤、sharpe等。

## 模型的部署

略。

# 数据挖掘工具

书中介绍了MATLAB、SAS、SPSS、WEKA、R等工具，并进行了比较。本书以Matlab作为示例。但我个人更倾向使用python。

