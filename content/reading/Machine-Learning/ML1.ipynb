{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[《机器学习》](https://book.douban.com/subject/1102235/),  作者: (美)Tom Mitchell ，译者: 曾华军 / 张银奎 / 等\n",
    "        \n",
    "#  机器学习第一章：引言\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "本书针对机器学习这个领域,描述了多种学习范型、算法、理论以及应用。机器学习从 本质上是一个多学科的领域。它吸取了人工智能、概率统计、计算复杂性理论、控制论、信 息论、哲学、生理学、神经生物学等学科的成果。表 1-2 归纳了这些学科中影响机器学习的 关键思想。本书的素材基于不同学科的成果,然而读者不必精通每一个学科。来自这些学科 的关键理论将使用非专业的词汇讲解,其中不熟悉的术语和概念会在需要时加以介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些学科和它们对机器学习的影响\n",
    "\n",
    "- 人工智能\n",
    "\n",
    "  学习概念的符号表示。作为搜索问题的机器学习。作为提高问题求解能力途径的学习。使用先验的知识和训练数据一起引导学习。\n",
    "\n",
    "- 贝叶斯方法\n",
    "\n",
    "  作为计算假设概率的基础的贝叶斯法则。朴素贝叶斯分类器。估计未观测到变量的值的算法。\n",
    "\n",
    "- 计算复杂性理论\n",
    "\n",
    "  不同学习任务中固有的复杂性的理论边界,以计算量、训练样例数量、出错数量等衡量。\n",
    "\n",
    "- 控制论\n",
    "\n",
    "  为了优化预定目标,学习对各种处理过程进行控制,学习预测被控制的过程的下一个状态。\n",
    "\n",
    "- 信息论\n",
    "\n",
    "  熵和信息内容的度量。学习的最小描述长度方法。编码假设时,它的最佳编码和与最佳训练序列的关系。\n",
    "\n",
    "- 哲学\n",
    "\n",
    "  “奥坎姆的剃刀”(Occam’s razor)1:最简单的假设是最好的。从观察到的数据泛化的理由分析。\n",
    "\n",
    "- 心理学和神经生物学\n",
    "\n",
    "  实践的幂定律(power law of practice),该定律指出对于很大范围内的学习问题,人们的反 应速度随着实践次数的幂级提高。激发人工神经网络的学习模式的神经生物学研究。\n",
    "\n",
    "- 统计学\n",
    "\n",
    "  在估计有限数据样本上的假设精度时出现的误差(例如偏差和方差)的刻画。置信区间, 统计检验。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
